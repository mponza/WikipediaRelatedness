import os
import json
from gensim import utils


def extract_json_pages(filename, filter_namespaces=False):
    with utils.smart_open(filename) as fin:
        for line in fin:
            document = json.loads(line.strip())

            title = document['wikiTitle']
            text = ' '.join(document['sentences'])
            wiki_id = str(document['wikiId'])

            yield title, text, wiki_id


def absolute_path(path_from_latent_gensim):
    WORKING_DIR = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(WORKING_DIR, path_from_latent_gensim)


LEMMING = True  # whether using lemmatization or not


WIKI_CORPUS = absolute_path('../../resources/wikipedia/wikipedia-w2v-linkCorpus.json.gz')
WIKI_LINKS = absolute_path('../../resources/wikipedia/wiki-links-sorted.gz')

GENSIM_DIR = absolute_path('../../../../data/processing/wikipedia/latent/gensim')

WIKI_STATS = 'wiki_corpus'  # name of the beginnig of each file generated by Genism
WIKI_FILENAME = os.path.join(GENSIM_DIR, WIKI_STATS + '/' + WIKI_STATS + '_')

WIKI_LDA_DIR = absolute_path('../../../../data/processing/wikipedia/latent/gensim/wiki_lda')
WIKI_LDA_MODEL = os.path.join(WIKI_LDA_DIR, 'lda_model')
WIKI_LDA_DOCS = os.path.join(WIKI_LDA_DIR, 'lda_wiki_docs.gz')

WIKI_SVD_DIR = absolute_path('../../../../data/processing/wikipedia/latent/svd')
